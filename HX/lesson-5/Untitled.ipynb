{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在实际工作中，FM和MF哪个应用的更多，为什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MF（Matrix Factorization）是一种将矩阵简化为其组成部分的方法。这种方法可以简化更复杂的矩阵运算，这些运算可以在分解的矩阵上执行，而不是在原始矩阵本身上执行。通常是基于SVD上实现的，通常MF我们都只考虑user和item特征。在面对多维度数据的是MF的使用就非常不方便了。\n",
    "- FM（Factorization Machine）是在LR的基础上引入交叉特征并且在对交叉特征进行矩阵分解。\n",
    "$$\n",
    "y(x) = \\frac{1}{2}\\sum_{f=1}^k\\Bigg(\\bigg(\\sum_{i=1}^kv_{i,f}x_i\\bigg)^2 - \\sum_{i=1}^nv_{i,f}^2x_i^2\\Bigg)\n",
    "$$\n",
    "###### 因此FM模型在此基础上可以方便地引入更多特征，模型拓展性强。因此FM模型比矩阵分解更灵活，能适用于更多场景中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM与FM有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 每个特征有唯一的一个隐向量表示，这个隐向量被用来学习与其他任何特征之间的影响。\n",
    " \n",
    " - 通过引入field的概念，FFM把相同性质的特征归于同一个field。每个特征会有几个不同的隐向量，fj 是第 j 个特征所属的field\n",
    " - FM是FFM的特例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFM相比于FM解决了哪些问题，原理是怎样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FM可以做特征组合，但是计算量大，一般只考虑2阶特征组合\n",
    "- DeepFM 的话既考虑低阶（1阶+2阶）又能考虑到高阶特征（2阶以上）\n",
    "- DeepFM = FM + DNN， 通过因子分解机（FM）既可以做1阶特征建模，也可以做2阶特征建模。然后在通过神经网络（DNN）提取高阶(high order)特征 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise工具中的baseline算法原理是怎样的？BaselineOnly和KNNBaseline有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- baseline 在计算用户评分时基于下面的基线公式\n",
    "$$\n",
    "b_{ui} = \\mu + b_u + b_i\n",
    "$$\n",
    "$b_u$ 用户对整体的偏差； $b_i$ 商品对整体的偏差\n",
    "- KNNBaseline 的基线算法\n",
    "$$\n",
    "\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)\\times(r_{\\nu i}-b_{ \\nu i})}{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)}\n",
    "$$\n",
    "- KNNBaseline 计算的时候会基于找到的K个临近样本进行基线运算。而baseline就基于自己的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于邻域的协同过滤都有哪些算法，请简述原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNNBasic\n",
    "$$\n",
    "\\hat{r}_{ui} =  \\frac{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)\\times r_{\\nu i}}{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)}\n",
    "$$\n",
    "基础协同过滤算法，可以是基于用户的和基于item的，r（ui）是user对另一个物品就j\n",
    "的评分。\n",
    "- KNNWithMeans\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_{u} + \\frac{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)\\times(r_{\\nu i}-\\mu_{ \\nu })}{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)}\n",
    "$$\n",
    "考虑了每个用户打分均值或者每个item打分的均值，去除参考用户打分整体偏高和偏低的影响\n",
    "- KNNWithZScore\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mu_{u} +\\sigma_u + \\frac{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)\\times(r_{\\nu i}-\\mu_{ \\nu })/\\sigma_\\nu}{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)}\n",
    "$$\n",
    "一种基本的协作过滤算法，其中考虑了每个用户的z分数归一化。\n",
    "- KNNBaseline\n",
    "$$\n",
    "\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)\\times(r_{\\nu i}-b_{ \\nu i})}{\\sum_{\\nu \\in N_i^k(u)}sim(u,\\nu)}\n",
    "$$\n",
    "从参考评分里减去偏差"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
