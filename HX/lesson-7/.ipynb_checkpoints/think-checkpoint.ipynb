{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、 什么是反向传播中的链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反向传播，是指在神经网络中将loss从输出端传递到输入端的过程。\n",
    "![2x2节点图](./2x2.jpg)\n",
    "- 如上图所示是一个2x2的神经网络权重矩阵 $A =  \\left\\{ \\begin{matrix}\n",
    "w_{1,1}&w_{2,1}\\\\ w_{1,2}&w_{2,2}\\end{matrix} \\right\\}$ \n",
    "- 将loss $e_1$ $e_2$反响传播我们可以的拿到下面的公式\n",
    "$$\n",
    "loss_{input_{1}} = e_1 * \\frac{w_{1,1}}{w_{1,1}+w_{2,1}} + e_2*\\frac{w_{2,2}}{w_{1,2}+w_{2,2}}\n",
    "$$\n",
    "$$\n",
    "loss_{input_{2}} = e_1 * \\frac{w_{2,1}}{w_{1,1}+w_{2,1}} + e_2*\\frac{w_{2,2}}{w_{1,2}+w_{2,2}}\n",
    "$$\n",
    "- 化简成矩阵相乘\n",
    "$$\n",
    "loss_{input} = \\left\\{ \\begin{matrix} \\frac{w_{1,1}}{w_{1,1}+w_{2,1}} & \\frac{w_{1,2}}{w_{1,2}+w_{2,2}} \\\\ \\frac{w_{2,1}}{w_{1,1}+w_{2,1}}& \\frac{w_{2,2}}{w_{1,2}+w_{2,2}} \\end{matrix} \\right\\} \\cdot \\left \\{ \\begin{matrix} e_1 \\\\ e_2 \\end{matrix} \\right\\}\n",
    "$$\n",
    "因为这些分数的分母只是归一化的因子，如果我们忽略这些因子，我们也仅仅失去后馈误差的大小，比例还是没有发生变化，所以可以化简为：\n",
    "$$\n",
    "loss_{input} = \\left\\{ \\begin{matrix} w_{1,1}&w_{1,2} \\\\ w_{2,1}&w_{2,2} \\end{matrix} \\right\\} \\cdot \\left \\{ \\begin{matrix} e_1 \\\\ e_2 \\end{matrix} \\right\\}\n",
    "$$\n",
    "\n",
    "- 最终可以发现反向传播的规律 $loss_{input} = A^T \\cdot loos_{output}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 请列举几种常见的激活函数，激活函数有什么作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果没有激活函数，每一层节点的输入都是上层输出的线性函数，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。\n",
    "- 常用激活函数\n",
    "    1. sigmoid激活函数：\n",
    "$$\n",
    "f(x)= \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "        - 特点：能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1。\n",
    "        - 缺点：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失。\n",
    "    2. tanh激活函数：\n",
    "$$\n",
    "f(x)= \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "        - 优点：解决了Sigmoid函数的不是zero-centered输出问题\n",
    "        - 缺点：梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。\n",
    "    3. relu激活函数：\n",
    "$$\n",
    "f(x) = max(0,x)\n",
    "$$\n",
    "        - 优点： 在正区间上解决了gradient vanishing问题，计算速度非常快，只需要判断输入是否大于0，计算速度非常快，只需要判断输入是否大于0。\n",
    "        - 缺点：输出不是zero-centered，某些神经元可能永远不会被激活，导致相应的参数永远不能被更新（Dead ReLU Problem）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 模型结构和特征工程存在问题。\n",
    "    - 在做action1 预测boston房价的时候因为计算loss的时候y值传递的时候没有reshape导致模型loss只能下降到84左右。\n",
    "2. 未进行归一化。\n",
    "    - 未进行归一化会导致尺度的不平衡，因此会导致误差变大，或者在同样的学习率下，左右两边摇摆不定。可以通过放缩，使得特征的数值分布更接近一些。\n",
    "3. 没有选择合适的激活函数和损失函数。\n",
    "    - 例如激活函数选用sigmoid，容易导致梯度消失和梯度爆炸。\n",
    "    - 在使用Relu激活函数的时候，当每一个神经元的输入X为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。\n",
    "    - 这种情况下我们就可以调整我们的激活函数和损失函数\n",
    "4. 没有选择合适的优化器和学习率\n",
    "    - 神经网络的优化器选取一般选取Adam，但是在有些情况下Adam难以训练，这时候需要使用如SGD之类的其他优化器。学习率决定了网络训练的速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率，然后再训练一段时间，这时候基本上就完全收敛了。\n",
    "5. 模型过拟合\n",
    "    - 当模型测试集loss不变或者上升，训练集loss下降的时候很有可能是模型过拟合了。\n",
    "    - 解决方案，早停法。或者添加正则项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
