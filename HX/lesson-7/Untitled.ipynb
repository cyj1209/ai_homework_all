{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = ToPILImage()\n",
    "root = '/Users/caoyongjun/Documents/ai/cifar-10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_norm_mean = (0.49139968, 0.48215827, 0.44653124)\n",
    "cifar_norm_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_norm_mean, cifar_norm_std),\n",
    "])\n",
    "train_data = torchvision.datasets.CIFAR10(root=root ,train=True,\n",
    "                                          transform=transform_train,download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root=root ,train=False,\n",
    "                                          transform=transform_test,download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9892, -1.9892, -1.9892,  ..., -1.9892, -1.9892, -1.9892],\n",
       "         [-1.9892, -1.9892, -1.9892,  ..., -1.9892, -1.9892, -1.9892],\n",
       "         [-1.9892, -1.9892, -1.9892,  ..., -1.9892, -1.9892, -1.9892],\n",
       "         ...,\n",
       "         [-1.9892,  0.7412,  0.2967,  ...,  0.1539,  1.0111,  1.3604],\n",
       "         [-1.9892,  1.2175,  0.9794,  ...,  0.7254,  1.4239,  0.9635],\n",
       "         [-1.9892,  1.4397,  1.0746,  ...,  1.2334,  1.2810, -0.0207]],\n",
       "\n",
       "        [[-1.9802, -1.9802, -1.9802,  ..., -1.9802, -1.9802, -1.9802],\n",
       "         [-1.9802, -1.9802, -1.9802,  ..., -1.9802, -1.9802, -1.9802],\n",
       "         [-1.9802, -1.9802, -1.9802,  ..., -1.9802, -1.9802, -1.9802],\n",
       "         ...,\n",
       "         [-1.9802,  0.0813, -0.5629,  ..., -0.3374,  0.5806,  0.9349],\n",
       "         [-1.9802,  0.5484,  0.0974,  ...,  0.1296,  0.9672,  0.5162],\n",
       "         [-1.9802,  0.8222,  0.2102,  ...,  0.7095,  0.8061, -0.4824]],\n",
       "\n",
       "        [[-1.7070, -1.7070, -1.7070,  ..., -1.7070, -1.7070, -1.7070],\n",
       "         [-1.7070, -1.7070, -1.7070,  ..., -1.7070, -1.7070, -1.7070],\n",
       "         [-1.7070, -1.7070, -1.7070,  ..., -1.7070, -1.7070, -1.7070],\n",
       "         ...,\n",
       "         [-1.7070, -0.5677, -1.4372,  ..., -1.0474, -0.4028,  0.0020],\n",
       "         [-1.7070, -0.4777, -1.3172,  ..., -0.6576, -0.1179, -0.3428],\n",
       "         [-1.7070, -0.4028, -1.4671,  ..., -0.1779, -0.1329, -0.9724]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([1, 6, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "for i,data in enumerate(train_loader):\n",
    "    if i==0:\n",
    "        inputs,label = data\n",
    "        print(inputs.size())\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CIFAR10 in module torchvision.datasets.cifar object:\n",
      "\n",
      "class CIFAR10(torchvision.datasets.vision.VisionDataset)\n",
      " |  CIFAR10(root, train=True, transform=None, target_transform=None, download=False)\n",
      " |  \n",
      " |  `CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where directory\n",
      " |          ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
      " |      train (bool, optional): If True, creates dataset from training set, otherwise\n",
      " |          creates from test set.\n",
      " |      transform (callable, optional): A function/transform that takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CIFAR10\n",
      " |      torchvision.datasets.vision.VisionDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root, train=True, transform=None, target_transform=None, download=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  download(self)\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  base_folder = 'cifar-10-batches-py'\n",
      " |  \n",
      " |  filename = 'cifar-10-python.tar.gz'\n",
      " |  \n",
      " |  meta = {'filename': 'batches.meta', 'key': 'label_names', 'md5': '5ff9...\n",
      " |  \n",
      " |  test_list = [['test_batch', '40351d587109b95175f43aff81a1287e']]\n",
      " |  \n",
      " |  tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
      " |  \n",
      " |  train_list = [['data_batch_1', 'c99cafc152244af753f735de768cd75f'], ['...\n",
      " |  \n",
      " |  url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3333333333333335"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400 /120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJZElEQVR4nAXB2Y8dWX0A4LP8TtWp9W59l97stt1ux4zGHhiDRiYJGfECLyhv+e/CPxBFCEWRIuUBIQUemJFRBpuJ8d7r7bvVvVV1Tp0934d/+o8/q6p1TPwwCndG6XiY7fXziDKIE0Rhvam0DYN+jzijlOq6jifcISdk0+uXKDitNEWMUlrkeZZljHGpdMAEEdBK24Dh5auX1XI55AiP+J4rcDJp/bpxIeBIdFpIZZxfUswhWOspgTiORddar3E3IhQZpRLgjdJrZ9M0w4RhyhAhojPWGAoxJIBRjO6O+Mm0NxkPkzTDGEvVdUYFjKMkQTYEr3rD1JoQscQ5RKNY6c5YnEYxZAmPYotbErxFmGKUZ2nTCmMNwajebYFjWxRwdjgYJZT5rllr54kUlkSo7OcQxdW2BkDDIq13re5a2ZmAcJ5lRkvigMWxcwYoVspELCLeqmaDXIgpst5vWwWDGJI47mXJuGTOO4cQBYoIUd4AAATvlAyU3N5WzrhaCOF0npRIOYo8wYHGXLZdykoIoeu0NNajUDVdJUwjbGcIjPu8YJRzSmhIksRY5xEOQWsbnDY+mOB0gKjWrXNUOG+dr1tzuW4Z8WWDzc1SbsWdvdPJ5AgXW7VZNU27rbvlVn443zoKcDDOysjmaYSDQSjg4JUUBOFR0csyvtsue2VZd+bj5bJRNPLoMAVg8sOqUoEyHHpl8fwHz3bXLojQ22NKQNOQmLHjWTGZTOe7DoZFArqKGaRxqqQx3vb7gxCCdsSYLs3zq4V6+3G7qK2w6G5C//kfvjjaz//t23d/fHNjvQYS6mohGlUUDDnMOYs4TTGzzt45PijWNUyGI7nuCIZGGKktYCqMIwhJo/uDUrvw7uJqvXMBIkpJyd0Ear5WD8vZ9ZDMq1sl9IvXr4n1JitRb4oI9Hpp4UOnTdC7k3EGg73xIE8IYdVuY9qGOOeRDwzynBvE//rudatazmMeQZKlA2q/fTO3GlRvNh5wjEpjO6FlK4K2FhuNMGIEB0IZgFUquACIMMwYQijmLEUZIEIIMcjHSW95U4vl5v6Qqw7xLH304JCozlK2222AbosoGw0ePHh45/2nP33/+jICFUJjLRCIWMS89x5hjAnIzmAjEbJtu9OGWMIbUe9EfXgMwdZ39/CDAyY6fHj2NArdZmuS/git6PFsv2rb+3/3sByk5eDxZlFvtlsWZSTExjvvkTOWYBRCAIddcDaEkPAkL9KrhXx/sQAWovlVN188nLCf/9PDt5fr4nC8N5rdLub9fkY8iwi9XVwCrxbV9eV1w1jaL72UIQDBBHvvCMaYEBcQ9Pu5Bds0XTBuW28/fpo3TZNwcv1+N+XR4eHd/sE9VnvE2dHTn/Cby8QuHOratttPx9p5nOVH2UHRn9Wrm9v5ymDWaYVIyGKuZcMiBnW1Al0zTBBFQKlotoMi62dcbnaTg9Hhk5/95UK/fqOf7w+rSk8fPCVIaLXoB7+7XSXa7A+HlYvZk4Gsrv/nP397cb6gEUMIy4AMIsQYoBg52QSECbIO041Bu10ISu/3sh9//fXRo6/+/df/OstyquXlu7ez+z/go9Ms1GJ9m/iBlmJZi/743mh2IpuSlMhFHSbYGI2tw8FZC4ADcsZgQoCgIA32aDhKZ6n90bOzx8+/2tw2sd3ePzry2M8mY9tZUWltrZHgUP728uK7v3zz/Cs9mo129S1L0d5J5glx2lmlt4tK1Sl466TyUZYDMEr06WzAE3Jy9/jp33+9/+jJn//46zvHg9lnn0fjB5D2RNfIXT2/Ot/ML5wRScH39tj51Yvp/qEVTZAKtxsXZMAhiVk0Y7sYA6OwqYXrcJImlITJKD2/rh786BdHn/8CoYGp217RG5990cLw5Ys/KdnudtXy8hN1mnM4vHf45OzU0ozRPosMdJ34eOmtswQ1lKajbHowAiW7NAbMKSM2OJvk9Ff/8qvnv/x5uTedv/srJbaqt4sP/3dVu9/95jd5wjrVzKa9ssjeX5xrYocHJ2eff4lcvK4uRIc30uIAnfRNCKHpHvcR+KCRd9h6GwzGgcflF19+GTP26s8vNldvlerqzfr8zasmJMx1OdCSZ+NB73p+Y40RdXP+/hNCL5um5hBsPFnZMkl4WiQJxLXYWW8BIe+tBpY66zSy097gv377H8Ppy8n+sRZbxuI8K4HQjLHZZCTrTULj1WJptCt4opvmby++uf7+tbISMeoIzY4ylGkSd9zbAUoef3YPvMcRUA4eERxo5rVZLm+axU1idh7R4WDUPxhbpy6vbgIKhIC2lmKW8dR6RK1HODi9JR7vxEbHsjhQbVLVXnctGZX39yYjQnDM4yQgmyZ8MpoEo0ZF1Iut3s51vRSijsshyUaPnjzzkOhAPIamEd6hiAJnYK19fbH45tXVd2+v13bH+8CiqGlsK0NWjKRwJAKilfIh8jQWRlLqU55kxThKe9PJXr1ZCG3Gx6fCx5/9+KePv3hGgLeNEkJijDHy15dXn97fNEImeToeTnDH8HU2uN074/eO+kdvXt3AdEzMaiWdb1sUiAOAshxFjMl2lzBAGr75wx/uP5pfXNwQgtOYURonSdY2Ukpprc6T+PkPz3hRWmqdEfK8IzWfpMUPzz6b9KffXr+HO8dRD/M352K+CNrFeQ6t2DrfUETWi1Xd2M5sadgW+WB+s75oOx/wdDzC3myqTZzF/V4RUaK0Q8BaRXTDMk9Oj2cHs9H5xXy1EFAOmFyIwYSiLF3OVac1RKXWyBtnnNrKTZbEnehkt9TGOeNCoM1OlGVSlj0pxXK1yfMME4JtiCCJOYoienJ6IkX4/e9f/e/rWwAOvIyGOQGpWOJ3G0COJHzimHeqilJgEFGaquC10SFgHFDQnesQA4aiuNpspDa9fgmEEIgEsvNlvWls3W7/+3ffzwWCpmGI5nnWsSRkMe/1fLOTzW7eCGc6V0QjzphVCoBEBLGYYkzSHAgg62yUQNlP1+u6Dr4cjoTVf/uw+v678+mwnB6liPi9XgEXH5GqeDG2PDG9HA2H0LSiqsRmFW1WiHrqQ3DOIe8IQphgCiAdCRYxb6xYOykcsKoR2qH1Tn54s6pWrW7drDd7fPdwJxE4tmeiZ8orYpe8h/tjPiB2KHy1TqollS04G6FAvPWd7KIookDrzsumY0EXpPBkZwzEWeAs7kf6Pup//jR79OTpyenpT74SF1fN/wMWt9uTtWIfgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FC23BB1F710>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CIFAR10 in module torchvision.datasets.cifar object:\n",
      "\n",
      "class CIFAR10(torchvision.datasets.vision.VisionDataset)\n",
      " |  CIFAR10(root, train=True, transform=None, target_transform=None, download=False)\n",
      " |  \n",
      " |  `CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where directory\n",
      " |          ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
      " |      train (bool, optional): If True, creates dataset from training set, otherwise\n",
      " |          creates from test set.\n",
      " |      transform (callable, optional): A function/transform that takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CIFAR10\n",
      " |      torchvision.datasets.vision.VisionDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root, train=True, transform=None, target_transform=None, download=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  download(self)\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  base_folder = 'cifar-10-batches-py'\n",
      " |  \n",
      " |  filename = 'cifar-10-python.tar.gz'\n",
      " |  \n",
      " |  meta = {'filename': 'batches.meta', 'key': 'label_names', 'md5': '5ff9...\n",
      " |  \n",
      " |  test_list = [['test_batch', '40351d587109b95175f43aff81a1287e']]\n",
      " |  \n",
      " |  tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
      " |  \n",
      " |  train_list = [['data_batch_1', 'c99cafc152244af753f735de768cd75f'], ['...\n",
      " |  \n",
      " |  url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.5961, 0.6000, 0.5686,  ..., 0.3882, 0.4078, 0.4078],\n",
      "          [0.6118, 0.6314, 0.6392,  ..., 0.3843, 0.3765, 0.3569],\n",
      "          [0.6000, 0.6196, 0.6431,  ..., 0.4078, 0.3961, 0.3725],\n",
      "          ...,\n",
      "          [0.3216, 0.3490, 0.3412,  ..., 0.4745, 0.5255, 0.5529],\n",
      "          [0.3804, 0.4314, 0.4000,  ..., 0.4863, 0.5137, 0.5333],\n",
      "          [0.3725, 0.3961, 0.3608,  ..., 0.6314, 0.7059, 0.7451]],\n",
      "\n",
      "         [[0.5961, 0.6078, 0.5843,  ..., 0.4667, 0.4745, 0.4706],\n",
      "          [0.6157, 0.6314, 0.6000,  ..., 0.4588, 0.4510, 0.4353],\n",
      "          [0.6039, 0.5882, 0.4078,  ..., 0.4824, 0.4745, 0.4549],\n",
      "          ...,\n",
      "          [0.3451, 0.3725, 0.3608,  ..., 0.4941, 0.5333, 0.5569],\n",
      "          [0.4039, 0.4431, 0.4118,  ..., 0.5098, 0.5294, 0.5451],\n",
      "          [0.4196, 0.4392, 0.3961,  ..., 0.6510, 0.7255, 0.7647]],\n",
      "\n",
      "         [[0.5961, 0.6039, 0.5765,  ..., 0.3647, 0.3725, 0.3686],\n",
      "          [0.6118, 0.6314, 0.6118,  ..., 0.3725, 0.3490, 0.3137],\n",
      "          [0.6078, 0.6000, 0.4431,  ..., 0.3922, 0.3765, 0.3490],\n",
      "          ...,\n",
      "          [0.3294, 0.3569, 0.3529,  ..., 0.4549, 0.4941, 0.4980],\n",
      "          [0.3961, 0.4353, 0.4039,  ..., 0.4745, 0.4706, 0.4824],\n",
      "          [0.3765, 0.4000, 0.3647,  ..., 0.5608, 0.5922, 0.6196]]],\n",
      "\n",
      "\n",
      "        [[[0.7373, 0.7333, 0.7333,  ..., 0.5216, 0.5020, 0.4902],\n",
      "          [0.7255, 0.7255, 0.7216,  ..., 0.6118, 0.5961, 0.5804],\n",
      "          [0.6941, 0.6941, 0.6941,  ..., 0.6784, 0.6706, 0.6627],\n",
      "          ...,\n",
      "          [0.4980, 0.4980, 0.4980,  ..., 0.4275, 0.4275, 0.4275],\n",
      "          [0.4941, 0.4941, 0.4941,  ..., 0.4275, 0.4314, 0.4353],\n",
      "          [0.4784, 0.4824, 0.4824,  ..., 0.4196, 0.4314, 0.4275]],\n",
      "\n",
      "         [[0.7137, 0.7137, 0.7098,  ..., 0.5569, 0.5373, 0.5255],\n",
      "          [0.7137, 0.7098, 0.7098,  ..., 0.6314, 0.6118, 0.5961],\n",
      "          [0.6902, 0.6863, 0.6902,  ..., 0.6745, 0.6667, 0.6627],\n",
      "          ...,\n",
      "          [0.5137, 0.5137, 0.5137,  ..., 0.4510, 0.4510, 0.4510],\n",
      "          [0.5137, 0.5098, 0.5098,  ..., 0.4510, 0.4549, 0.4588],\n",
      "          [0.4941, 0.4980, 0.4980,  ..., 0.4431, 0.4549, 0.4510]],\n",
      "\n",
      "         [[0.7529, 0.7529, 0.7490,  ..., 0.5686, 0.5529, 0.5412],\n",
      "          [0.7490, 0.7451, 0.7451,  ..., 0.6235, 0.6039, 0.5961],\n",
      "          [0.7216, 0.7216, 0.7216,  ..., 0.6549, 0.6471, 0.6431],\n",
      "          ...,\n",
      "          [0.5255, 0.5255, 0.5255,  ..., 0.4431, 0.4431, 0.4431],\n",
      "          [0.5255, 0.5216, 0.5216,  ..., 0.4431, 0.4471, 0.4510],\n",
      "          [0.5059, 0.5098, 0.5098,  ..., 0.4353, 0.4471, 0.4431]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.9922, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 0.9922, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 0.9922, 0.9961,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 0.9961, 0.9961,  ..., 1.0000, 0.9961, 0.9961],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 0.9961, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.8431, 0.8314, 0.8157,  ..., 0.6392, 0.6275, 0.6353],\n",
      "          [0.8235, 0.8078, 0.8000,  ..., 0.6314, 0.6118, 0.5922],\n",
      "          [0.8157, 0.8157, 0.8118,  ..., 0.5922, 0.5882, 0.5882],\n",
      "          ...,\n",
      "          [0.6510, 0.4980, 0.5961,  ..., 0.6392, 0.6824, 0.6549],\n",
      "          [0.6980, 0.5647, 0.5059,  ..., 0.6510, 0.6431, 0.6275],\n",
      "          [0.6039, 0.5373, 0.5098,  ..., 0.6000, 0.5922, 0.5922]],\n",
      "\n",
      "         [[0.8353, 0.8235, 0.8118,  ..., 0.6902, 0.6784, 0.6863],\n",
      "          [0.8196, 0.8039, 0.7961,  ..., 0.6784, 0.6627, 0.6431],\n",
      "          [0.8118, 0.8118, 0.8078,  ..., 0.6392, 0.6353, 0.6353],\n",
      "          ...,\n",
      "          [0.6510, 0.5373, 0.6431,  ..., 0.6039, 0.6471, 0.6196],\n",
      "          [0.7059, 0.6039, 0.5490,  ..., 0.6157, 0.6078, 0.5922],\n",
      "          [0.6157, 0.5765, 0.5490,  ..., 0.5647, 0.5569, 0.5569]],\n",
      "\n",
      "         [[0.8588, 0.8471, 0.8353,  ..., 0.7529, 0.7451, 0.7529],\n",
      "          [0.8431, 0.8314, 0.8196,  ..., 0.7412, 0.7216, 0.7059],\n",
      "          [0.8431, 0.8431, 0.8392,  ..., 0.7020, 0.6941, 0.6980],\n",
      "          ...,\n",
      "          [0.6902, 0.5686, 0.6784,  ..., 0.6235, 0.6667, 0.6392],\n",
      "          [0.7490, 0.6353, 0.5843,  ..., 0.6353, 0.6275, 0.6118],\n",
      "          [0.6667, 0.6118, 0.5843,  ..., 0.5843, 0.5765, 0.5765]]]]), tensor([2, 8, 6, 0])]\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
