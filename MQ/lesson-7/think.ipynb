{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、CNN中的参数共享指的是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络中我们主要学习的就是每个神经元的节点，例如在全连接层中第n-1层中有$k_{n-1}$个神经元，第n层中有$k_{n}$个节点，那么链接的权重就有 $(k_{n-1}+1) * k_{n}$个。这也就是要学习的参数。\n",
    "- CNN中 当我们指定了卷积核的size和步长之后我们的链接方式就确定了，我们会用指定的卷积核按照步长来扫描featureMap，得到输出的FeatureMap，这这次扫描中我们是同一个卷积核，所以我们说是共享的权重。例如在第N层CNN中，我们又 k个size为（h，w）的卷积核，实际上我们要训练的参数就是$(h*w+1)*k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、为什么会用到batch normalization ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BN就是通过一定的规范化手段，把每层神经网络任意神经元　这个输入值的分布强行拉回到均值为0方差为1的标准正态分布\n",
    "- 使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、使用dropout可以解决什么问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout层是Neural Network中有着特征功能的层，主要针对过拟合问题。dropout层从一些层中随机丢弃掉一些激活神经元（在Forward pass的过程中设置这些集合为0）。\n",
    "- 这样的目的主要是强制增加网络的冗余性，也就是说使得模型能够在一些激活神经元丢失的情况下仍然保持分类的正确性。从而使得模型不会对训练数据过于拟合，减少过拟合问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
